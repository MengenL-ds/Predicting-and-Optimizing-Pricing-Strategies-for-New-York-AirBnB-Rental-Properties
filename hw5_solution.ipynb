{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Table of contents\n",
    "1. [Understanding the problem](#1)\n",
    "2. [Data splitting](#2)\n",
    "3. [EDA](#3)\n",
    "4. [Feature engineering](#4)\n",
    "5. [Preprocessing and transformations](#5) \n",
    "6. [Baseline model](#6)\n",
    "7. [Linear models](#7)\n",
    "8. [Different models](#8)\n",
    "9. [Feature selection](#9)\n",
    "10. [Hyperparameter optimization](#10)\n",
    "11. [Interpretation and feature importances](#11) \n",
    "12. [Results on the test set](#12)\n",
    "13. [Summary of the results](#13)\n",
    "14. [Your takeaway from the course](#15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import tests_hw5\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Introduction <a name=\"in\"></a>\n",
    "\n",
    "In this homework you will be working on an open-ended mini-project, where you will put all the different things you have learned so far together to solve an interesting problem.\n",
    "\n",
    "A few notes and tips when you work on this mini-project: \n",
    "\n",
    "#### Tips\n",
    "1. This mini-project is open-ended, and while working on it, there might be some situations where you'll have to use your own judgment and make your own decisions (as you would be doing when you work as a data scientist). Make sure you explain your decisions whenever necessary. \n",
    "2. **Do not include everything you ever tried in your submission** -- it's fine just to have your final code. That said, your code should be reproducible and well-documented. For example, if you chose your hyperparameters based on some hyperparameter optimization experiment, you should leave in the code for that experiment so that someone else could re-run it and obtain the same hyperparameters, rather than mysteriously just setting the hyperparameters to some (carefully chosen) values in your code. \n",
    "3. If you realize that you are repeating a lot of code try to organize it in functions. Clear presentation of your code, experiments, and results is the key to be successful in this lab. You may use code from lecture notes or previous lab solutions with appropriate attributions. \n",
    "\n",
    "#### Assessment\n",
    "We plan to grade fairly and leniently. We don't have some secret target score that you need to achieve to get a good grade. **You'll be assessed on demonstration of mastery of course topics, clear presentation, and the quality of your analysis and results.** For example, if you just have a bunch of code and no text or figures, that's not good. If you do a bunch of sane things and get a lower accuracy than your friend, don't sweat it.\n",
    "\n",
    "\n",
    "#### A final note\n",
    "Finally, this style of this \"project\" question is different from other assignments. It'll be up to you to decide when you're \"done\" -- in fact, this is one of the hardest parts of real projects. But please don't spend WAY too much time on this... perhaps \"a few hours\" (15-20 hours???) is a good guideline for this project . Of course if you're having fun you're welcome to spend as much time as you want! But, if so, try not to do it out of perfectionism or getting the best possible grade. Do it because you're learning and enjoying it. Students from the past cohorts have found such kind of labs useful and fun and I hope you enjoy it as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 1. Pick your problem and explain the prediction problem <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:3}\n",
    "\n",
    "In this mini project, you have the option to choose on which dataset you will be working on. The tasks you will need to carry on will be similar, independently of your choice.\n",
    "\n",
    "### Option 1\n",
    "You can choose to work on a classification problem of predicting whether a credit card client will default or not. \n",
    "For this problem, you will use [Default of Credit Card Clients Dataset](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). In this data set, there are 30,000 examples and 24 features, and the goal is to estimate whether a person will default (fail to pay) their credit card bills; this column is labeled \"default.payment.next.month\" in the data. The rest of the columns can be used as features. You may take some ideas and compare your results with [the associated research paper](https://www.sciencedirect.com/science/article/pii/S0957417407006719), which is available through [the UBC library](https://www.library.ubc.ca/). \n",
    "\n",
    "\n",
    "### Option 2\n",
    "You can choose to work on a regression problem using a [dataset](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data) of New York City Airbnb listings from 2019. As usual, you'll need to start by downloading the dataset, then you will try to predict `reviews_per_month`, as a proxy for the popularity of the listing. Airbnb could use this sort of model to predict how popular future listings might be before they are posted, perhaps to help guide hosts create more appealing listings. In reality they might instead use something like vacancy rate or average rating as their target, but we do not have that available here.\n",
    "\n",
    "> Note there is an updated version of this dataset with more features available [here](http://insideairbnb.com/). The features were are using in `listings.csv.gz` for the New York city datasets. You will also see some other files like `reviews.csv.gz`. For your own interest you may want to explore the expanded dataset and try your analysis there. However, please submit your results on the dataset obtained from Kaggle.\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Spend some time understanding the options and pick the one you find more interesting (it may help spending some time looking at the documentation available on Kaggle for each dataset).\n",
    "2. After making your choice, focus on understanding the problem and what each feature means, again using the documentation on the dataset page on Kaggle. Write a few sentences on your initial thoughts on the problem and the dataset. \n",
    "3. Download the dataset and read it as a pandas dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "The solution included in this notebook is for the Credit Card Clients Dataset problem.\n",
    "\n",
    "This is a binary classification problem. The task is to predict whether a credit card client will default or not. The dataset is of moderate size. The number of features is rather small. I would consider this as a small dimensional problem. All features are numerically encoded. That said, some features such as sex and marriage seem more like categorical features. dddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/UCI_Credit_Card.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# BEGIN SOLUTION\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/UCI_Credit_Card.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# END SOLUTION\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cpsc330/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cpsc330/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cpsc330/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cpsc330/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/cpsc330/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/UCI_Credit_Card.csv'"
     ]
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "df = pd.read_csv(\"data/UCI_Credit_Card.csv\")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 2. Data splitting\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train (70%) and test (30%) portions with `random_state=123`.\n",
    "\n",
    "> If your computer cannot handle training on 70% training data, make the test split bigger.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=123)\n",
    "train_df.shape\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 3. EDA <a name=\"3\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Perform exploratory data analysis on the train set.\n",
    "2. Include at least two summary statistics and two visualizations that you find useful, and accompany each one with a sentence explaining it.\n",
    "3. Summarize your initial observations about the data. \n",
    "4. Pick appropriate metric/metrics for assessment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "# BEGIN PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "print(\"n=%d, d=%d\" % train_df.shape)\n",
    "train_df.describe()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "train_df.info()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "Seems like there are no missing values and all the columns are encoded as numeric columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "print(\n",
    "    \"Fraction that default:\\n\",\n",
    "    train_df[\"default.payment.next.month\"].value_counts(normalize=True),\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "We have a class imbalance. Both classes seem important here and I am going to pick macro-average f1 score as our evaluation metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "from sklearn.metrics import f1_score, make_scorer, recall_score\n",
    "\n",
    "custom_scorer = make_scorer(f1_score, average=\"macro\")\n",
    "scoring_metric = custom_scorer\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "np.max(train_df, axis=0)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "np.min(train_df, axis=0)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "X_train, y_train = (\n",
    "    train_df.drop(columns=[\"default.payment.next.month\"]),\n",
    "    train_df[\"default.payment.next.month\"],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    test_df.drop(columns=[\"default.payment.next.month\"]),\n",
    "    test_df[\"default.payment.next.month\"],\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "import seaborn as sns\n",
    "cor = pd.concat((y_train, X_train), axis=1).iloc[:, :30].corr()\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "Seems like all `PAY_\\d*` features and `BILL_AMT\\d*` features are highly correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "sns.jointplot(x=\"BILL_AMT1\", y=\"PAY_AMT1\", data=train_df);\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "train_df.hist(figsize=(20, 20), bins=20);\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "We see quite a few outliers for features such as `EDUCATION`, `MARRIAGE`, and `PAY_\\d*` features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "Some initial observations:\n",
    "    \n",
    "- We have very few features.\n",
    "- We have class imbalance and we need to deal with it. We have chosen macro average f1 as our target metric so that both classes get equal weight. \n",
    "- The feature ranges are very different, so we'll need to standardize. \n",
    "- We have a number of collinear features.\n",
    "- We have quite a few outliers. \n",
    "- The data is messy / doesn't always correspond to the data description. \n",
    "  - What are education levels 5 and 6?\n",
    "  - What does it mean for PAY_* to be -2? Or 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 4. Feature engineering <a name=\"4\"></a>\n",
    "<hr>\n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out feature engineering. In other words, extract new features relevant for the problem and work with your new feature set in the following exercises. You may have to go back and forth between feature engineering and preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 5. Preprocessing and transformations <a name=\"5\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Identify different feature types and the transformations you would apply on each feature type. \n",
    "2. Define a column transformer, if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# Let's identify numeric and categorical features\n",
    "drop_features = [\"ID\"]\n",
    "numeric_features = [\n",
    "    \"LIMIT_BAL\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\",\n",
    "    \"BILL_AMT1\",\n",
    "    \"BILL_AMT2\",\n",
    "    \"BILL_AMT3\",\n",
    "    \"BILL_AMT4\",\n",
    "    \"BILL_AMT5\",\n",
    "    \"BILL_AMT6\",\n",
    "    \"PAY_AMT1\",\n",
    "    \"PAY_AMT2\",\n",
    "    \"PAY_AMT3\",\n",
    "    \"PAY_AMT4\",\n",
    "    \"PAY_AMT5\",\n",
    "    \"PAY_AMT6\",\n",
    "    \"AGE\",\n",
    "]\n",
    "binary_features = [\"SEX\"]\n",
    "categorical_features = [\"EDUCATION\", \"MARRIAGE\"]\n",
    "target = \"default.payment.next.month\"\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (OneHotEncoder(drop=\"if_binary\"), binary_features),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "preprocessor.fit(X_train)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "# feature_names = (\n",
    "#     numeric_features\n",
    "#     + binary_features   \n",
    "#     + preprocessor.named_transformers_[\"onehotencoder-2\"].get_feature_names_out().tolist()\n",
    "# )\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores.iloc[i], std_scores.iloc[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)\n",
    "\n",
    "results = {}\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 6. Baseline model <a name=\"6\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try `scikit-learn`'s baseline model and report results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "dummy = DummyClassifier(strategy=\"stratified\")\n",
    "results[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    ")\n",
    "pd.DataFrame(results)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "cross_val_score(dummy, X_train, y_train)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 7. Linear models <a name=\"7\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try a linear model as a first real attempt. \n",
    "2. Carry out hyperparameter tuning to explore different values for the complexity hyperparameter. \n",
    "3. Report cross-validation scores along with standard deviation. \n",
    "4. Summarize your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "pipe_lr = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    ")\n",
    "results[\"logreg\"] = mean_std_cross_val_scores(\n",
    "    pipe_lr, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    ")\n",
    "pd.DataFrame(results)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "from scipy.stats import lognorm, loguniform, randint\n",
    "\n",
    "param_grid = {\"logisticregression__C\": loguniform(1e-3, 1e3)}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_lr,\n",
    "    param_grid,\n",
    "    n_iter=50,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    "    scoring=scoring_metric,\n",
    "    random_state=123,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train);\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "print(\"Best hyperparameter values: \", random_search.best_params_)\n",
    "print(\"Best score: %0.3f\" % (random_search.best_score_))\n",
    "\n",
    "pd.DataFrame(random_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_train_score\",\n",
    "        \"mean_test_score\",\n",
    "        \"param_logisticregression__C\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "        \"std_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index()[:10]\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "best_logreg = random_search.best_estimator_\n",
    "results[\"logreg (tuned)\"] = mean_std_cross_val_scores(\n",
    "    best_logreg, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    ")\n",
    "\n",
    "pd.DataFrame(results).T\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "- Logistic regression scores are better than the dummy classifier scores. \n",
    "- Optimizing the regularization hyperparameter of logistic regression improved the validation scores slightly (from 0.625 to 0.629) but not by much. \n",
    "- In both cases it seems like we are underfitting; there is not much gap between train and validation scores. Probably non-linear models might be a better choice here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 8. Different models <a name=\"8\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Try at least 3 other models aside from a linear model. One of these models should be a tree-based ensemble model. \n",
    "2. Summarize your results in terms of overfitting/underfitting and fit and score times. Can you beat a linear model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "ratio = np.bincount(y_train)[0] / np.bincount(y_train)[1]\n",
    "ratio\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# You need to install xgboost for the following code to run: conda install conda-forge::xgboost\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models = {\n",
    "    \"RBF SVM\": SVC(),\n",
    "    \"random forest\": RandomForestClassifier(class_weight=\"balanced\", random_state=2),\n",
    "    \"xgboost\": XGBClassifier(scale_pos_weight=ratio, random_state=2),\n",
    "    \"lgbm\": LGBMClassifier(scale_pos_weight=ratio, random_state=2, verbosity=-1),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = make_pipeline(preprocessor, model)\n",
    "    results[name] = mean_std_cross_val_scores(\n",
    "        pipe, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    "    )\n",
    "\n",
    "pd.DataFrame(results).T\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "- I am using four non-linear models here: RBF SVM and three tree-based models. \n",
    "- We are trying all models with default hyperparameters. \n",
    "- Similar to logistic regression, SVC also seems to underfit; the gap between train and test scores are not large. Also, as expected, it takes longer to `fit` compared to other models. Let's abandon it. \n",
    "\n",
    "- LGBM seems to be the best performing model among the tree-based models. It also seems to be much faster and overfitting less compared to random forest and xgboost. That said, it's std is higher than the other two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "del models[\"RBF SVM\"]\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 9. Feature selection <a name=\"9\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to select relevant features. You may try `RFECV` or forward selection for this. Do the results improve with feature selection? Summarize your results. If you see improvements in the results, keep feature selection in your pipeline. If not, you may abandon it in the next exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectFromModel\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = make_pipeline(\n",
    "        preprocessor,\n",
    "        SelectFromModel(\n",
    "            LogisticRegression(solver=\"liblinear\", penalty=\"l1\", max_iter=1000)\n",
    "        ),\n",
    "        model,\n",
    "    )\n",
    "    results[name + \"+ feat_sel\"] = mean_std_cross_val_scores(\n",
    "        pipe, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    "    )\n",
    "# END SOLUTION    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "pd.DataFrame(results).T\n",
    "# END SOLUTION    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "Let's examine how many features were selected for the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "pipe_random_forest = make_pipeline(\n",
    "    preprocessor,\n",
    "    SelectFromModel(\n",
    "        LogisticRegression(solver=\"liblinear\", penalty=\"l1\", max_iter=1000)\n",
    "    ),\n",
    "    models[\"random forest\"],\n",
    ")\n",
    "\n",
    "pipe_random_forest.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Number of features selected: \",\n",
    "    pipe_random_forest.named_steps[\"randomforestclassifier\"].n_features_in_,\n",
    ")\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "Total 29 features were selected by L2 regularization. We see very small improvements in the result with logistic regression and L2 regularization. But not a lot. Also, we do not have a large number of features. So let's abandon feature selection from the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 10. Hyperparameter optimization <a name=\"10\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Make some attempts to optimize hyperparameters for the models you've tried and summarize your results. In at least one case you should be optimizing multiple hyperparameters for a single model. You may use `sklearn`'s methods for hyperparameter optimization or fancier Bayesian optimization methods. \n",
    "  - [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)   \n",
    "  - [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "  - [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "#### Random forest hyperparameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION    \n",
    "param_grid_rf = {\n",
    "    \"randomforestclassifier__n_estimators\": randint(low=10, high=100),\n",
    "    \"randomforestclassifier__max_depth\": randint(low=2, high=20),\n",
    "}\n",
    "\n",
    "pipe_random_forest = make_pipeline(preprocessor, models[\"random forest\"])\n",
    "# END SOLUTION    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION    \n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    pipe_random_forest,\n",
    "    param_grid_rf,\n",
    "    n_iter=50,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    "    scoring=scoring_metric,\n",
    "    random_state=123,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "print(\"Best hyperparameter values: \", random_search_rf.best_params_)\n",
    "print(\"Best score: %0.3f\" % (random_search_rf.best_score_))\n",
    "\n",
    "pd.DataFrame(random_search_rf.cv_results_)[\n",
    "    [\n",
    "        \"mean_train_score\",\n",
    "        \"mean_test_score\",\n",
    "        \"param_randomforestclassifier__n_estimators\",\n",
    "        \"param_randomforestclassifier__max_depth\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "        \"std_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index()[:10]\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "results[\"random forest (tuned)\"] = mean_std_cross_val_scores(\n",
    "    best_rf_model, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    ")\n",
    "pd.DataFrame(results).T\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "#### LGBM hyperparameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "param_grid_lgbm = {\n",
    "    \"lgbmclassifier__n_estimators\": randint(10, 100),\n",
    "    # \"lgbmclassifier__max_depth\": randint(low=2, high=20),\n",
    "    \"lgbmclassifier__learning_rate\": [0.01, 0.1],\n",
    "    \"lgbmclassifier__subsample\": [0.5, 0.75, 1],\n",
    "}\n",
    "\n",
    "pipe_lgbm = make_pipeline(\n",
    "    preprocessor,\n",
    "    models[\"lgbm\"],\n",
    ")\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "random_search_lgbm = RandomizedSearchCV(\n",
    "    pipe_lgbm,\n",
    "    param_grid_lgbm,\n",
    "    n_iter=50,\n",
    "    verbose=1,\n",
    "    n_jobs=1,\n",
    "    scoring=scoring_metric,\n",
    "    random_state=123,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "random_search_lgbm.fit(X_train, y_train)\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "print(\"Best hyperparameter values: \", random_search_lgbm.best_params_)\n",
    "print(\"Best score: %0.3f\" % (random_search_lgbm.best_score_))\n",
    "\n",
    "pd.DataFrame(random_search_lgbm.cv_results_)[\n",
    "    [\n",
    "        \"mean_train_score\",\n",
    "        \"mean_test_score\",\n",
    "        \"param_lgbmclassifier__n_estimators\",\n",
    "        \"param_lgbmclassifier__learning_rate\",\n",
    "        \"param_lgbmclassifier__subsample\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index()[:10]\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "best_lgbm_model = random_search_lgbm.best_estimator_\n",
    "results[\"lgbm (tuned)\"] = mean_std_cross_val_scores(\n",
    "    best_lgbm_model, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    ")\n",
    "pd.DataFrame(results).T\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "**Summary of observations**\n",
    "\n",
    "Hyperparameter optimization seems to help with random forests as well as LightGBM. The scores for both models seem very similar. But we pick LightGBM because\n",
    "- it seems to be less overfitting \n",
    "- it's much faster\n",
    "- the standard deviation is smaller compared to random forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 11. Interpretation and feature importances <a name=\"1\"></a>\n",
    "<hr>\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Use the methods we saw in class (e.g., `shap`) (or any other methods of your choice) to examine the most important features of one of the non-linear models. \n",
    "2. Summarize your observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "import shap\n",
    "\n",
    "preprocessor.fit(X_train, y_train)\n",
    "\n",
    "X_train_enc = pd.DataFrame(\n",
    "    data=preprocessor.transform(X_train),\n",
    "    columns=feature_names,\n",
    "    index=X_train.index,\n",
    ")\n",
    "\n",
    "X_train_enc.head()\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_tuned = LGBMClassifier(\n",
    "    scale_pos_weight=ratio,\n",
    "    random_state=2,\n",
    "    learning_rate=random_search_lgbm.best_params_[\"lgbmclassifier__learning_rate\"],\n",
    "    n_estimators=random_search_lgbm.best_params_[\"lgbmclassifier__n_estimators\"],\n",
    "    subsample=random_search_lgbm.best_params_[\"lgbmclassifier__subsample\"],\n",
    ")\n",
    "\n",
    "lgbm_tuned.fit(X_train_enc, y_train)\n",
    "lgbm_explainer = shap.TreeExplainer(lgbm_tuned)\n",
    "lgbm_shap_values = lgbm_explainer.shap_values(X_train_enc)\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "values = np.abs(lgbm_shap_values[0]).mean(0)\n",
    "pd.DataFrame(data=values, index=feature_names, columns=[\"SHAP\"]).sort_values(\n",
    "    by=\"SHAP\", ascending=False\n",
    ")[:10]\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "shap.dependence_plot(\"standardscaler__LIMIT_BAL\", lgbm_shap_values, X_train_enc)\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "As `LIMIT_BAL` increases, SHAP values for class 1 decrease, suggesting that class is likely to be 0 (non default) with higher values for LIMIT_BAL, which makes sense.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "shap.summary_plot(lgbm_shap_values, X_train_enc)\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "X_test_enc = pd.DataFrame(\n",
    "    data=preprocessor.transform(X_test),\n",
    "    columns=feature_names,\n",
    "    index=X_test.index,\n",
    ")\n",
    "\n",
    "test_lgbm_shap_values = lgbm_explainer.shap_values(X_test_enc)\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_tuned.predict_proba(X_test_enc)[1]\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_tuned.predict(X_test_enc, raw_score=True)[1]\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_explainer.expected_value\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value,\n",
    "    test_lgbm_shap_values[1, :],\n",
    "    X_test_enc.iloc[1, :],\n",
    "    matplotlib=True,\n",
    ")\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "**Summary of observations**\n",
    "- From the analysis above we observe that **PAY_\\d{0,2,3,4}**, **PAY_AMT**, **LIM_BAL** features seem to be one of the most important features with `PAY_0` being the topmost feature. \n",
    "- The SHAP dependence plot demonstrates that the class is likely to be 0 (non default) for higher values for LIMIT_BAL, which makes sense.\n",
    "- The features `EDUCATION` and `SEX` doesn't seem to influence the prediction much. This might be because of the noise in the `EDUCATION` column; there are a number of unknown values in this column.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 12. Results on the test set <a name=\"12\"></a>\n",
    "<hr>\n",
    "\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try your best performing model on the test data and report test scores. \n",
    "2. Do the test scores agree with the validation scores from before? To what extent do you trust your results? Do you think you've had issues with optimization bias? \n",
    "3. Take one or two test predictions and explain these individual predictions (e.g., with SHAP force plots).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_model = random_search_lgbm.best_estimator_\n",
    "print(\n",
    "    \"Grid Search best model validation score: %0.3f\" % (random_search_lgbm.best_score_)\n",
    ")\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "print(\n",
    "    \"Macro-average f1 score on the test set: %0.3f\"\n",
    "    % (f1_score(y_test, predictions, average=\"macro\"))\n",
    ")\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    best_model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"No default\", \"Default on payment\"],\n",
    "    values_format=\"d\",\n",
    "    cmap=plt.cm.Blues,\n",
    ")\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "print(\n",
    "    classification_report(\n",
    "        y_test, predictions, target_names=[\"No default\", \"Default on payment\"]\n",
    "    )\n",
    ")\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "The macro-average f1-score (0.695) on the held-out test set is pretty much in line with the macro-average f1-score validation score (0.707). So there doesn't seem to be severe optimization bias here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "Let's explain a default and a non-default test prediction using SHAP force plot. Let's first get indices for default and non-default examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "non_default_ind = y_test_reset[y_test_reset == 0].index.tolist()\n",
    "default_ind = y_test_reset[y_test_reset == 1].index.tolist()\n",
    "\n",
    "ex_non_default_index = non_default_ind[0]\n",
    "ex_default_index = default_ind[0]\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "**Explanation of a non-default prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_tuned.predict_proba(X_test_enc)[ex_non_default_index]\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_tuned.predict(X_test_enc, raw_score=True)[ex_non_default_index]\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_explainer.expected_value\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value,\n",
    "    test_lgbm_shap_values[ex_non_default_index, :],\n",
    "    X_test_enc.iloc[ex_non_default_index, :],\n",
    "    matplotlib=True,\n",
    ")\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "- The raw model score is -0.84, which is smaller than the base value -0.5594 and so the prediction is that the user is not likely to default on credit payment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "**Explanation of a default prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_tuned.predict_proba(X_test_enc)[ex_default_index]\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_tuned.predict(X_test_enc, raw_score=True)[ex_default_index]\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "lgbm_explainer.expected_value\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value,\n",
    "    test_lgbm_shap_values[ex_default_index, :],\n",
    "    X_test_enc.iloc[ex_default_index, :],\n",
    "    matplotlib=True,\n",
    ")\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "- The raw model score is 1.00, which is greater than the base value -0.5594 and so the prediction is that the user is likely to default on the credit payment (class 1). \n",
    "- Positive values for PAY_* variables seem to push the prediction towards a higher value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 13. Summary of results <a name=\"13\"></a>\n",
    "<hr>\n",
    "rubric={points:12}\n",
    "\n",
    "Imagine that you want to present the summary of these results to your boss and co-workers. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a table summarizing important results. \n",
    "2. Write concluding remarks.\n",
    "3. Discuss other ideas that you did not try but could potentially improve the performance/interpretability . \n",
    "3. Report your final test score along with the metric you used at the top of this notebook in the [Submission instructions section](#si)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "pd.DataFrame(results).T\n",
    "summary_df = pd.DataFrame(results).T\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION  \n",
    "comments = {\n",
    "    \"dummy\": \"Baseline of 0.50 macro-average f1 score.\",\n",
    "    \"logreg\": \"Improvement over the baseline but underfitting.\",\n",
    "    \"logreg (tuned)\": \"Slight improvement but still underfitting.\",\n",
    "    \"RBF SVM\": \"Improvement over tuned logistic regression but slow.\",\n",
    "    \"random forest\": \"Improvement over tuned logistic regression but overfitting.\",\n",
    "    \"xgboost\": \"Best results so far and less overfitting compared to random forest.\",\n",
    "    \"lgbm\": \"Improvement over xgboost and less overfitting.\",\n",
    "    \"random forest+ feat_sel\": \"Feature selection with L1 regularization helps a tiny bit. Selects 29 features.\",\n",
    "    \"xgboost+ feat_sel\": \"Very tiny improvement with L1 feature selection.\",\n",
    "    \"lgbm+ feat_sel\": \"No improvemnt with L1 feature selection.\",\n",
    "    \"random forest (tuned)\": \"Hyperparameter optimization helped! Best results so far.\",\n",
    "    \"lgbm (tuned)\": \"Hyperparameter optimization helped. Best results overall! The scores are very similar to random forest scores but picking this as the best model for its speed.\",\n",
    "}\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "summary_df[\"comments\"] = comments.values()\n",
    "summary_df\n",
    "# END SOLUTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_assign_solution_cell"
    ]
   },
   "source": [
    "#### Concluding remarks\n",
    "\n",
    "All our models beat the baseline. Our best model was LightGBM classifier with tuned hyperparameters. It achieved cross-validation macro-average f1 score of 0.707. The scores do not seem to overfit much; the gap between mean train score (0.719) and mean cross-validation score (0.707) is not big. These scores are very similar to the tuned random forest. But random forest seems to overfit. Also, it's much slower than LightGBM. So picked LightGBM model as our final model.  \n",
    "\n",
    "We observed the macro-average f1 score of 0.695 using this model on the held out test set, which is in line with mean cross-validation macro-average f1-score (0.707). So there doesn't seem to be severe optimization bias here.\n",
    "\n",
    "We observed that L1 feature selection helped a tiny bit for random forests. But we did not observe any improvement in LightGBM scores with feature selection in the pipeline. In general, we have small number of features in this problem and feature selection doesn't seem crucial. \n",
    "\n",
    "Our analysis of feature importances shows that our `PAY_\\d{0,2}`, `LIMIT_*`, and `PAY_AMT*` variables seems to be most important features. Although `SEX` feature doesn't show up as one of the most important features, depending upon the context it might be a good idea to drop this feature from our analysis. \n",
    "\n",
    "#### Other ideas\n",
    "    \n",
    "- Preprocessing and feature engineering     \n",
    "    - The `BILL_AMT*` and `PAY_AMT*` variables are the bill amount, and amount paid, respectively. We could try making new features by subtracting or otherwise combining these, which would be the amount you paid relative to the amount owed.\n",
    "    - There are a number of collinear features in the dataset, especially, our `PAY_\\d{0,5}` features, which are one of the topmost important features. We could create new features by combining these features.     \n",
    "    - More data cleaning would probably help. \n",
    "    - In my opinion, data cleaning and feature engineering are very important here.\n",
    "    \n",
    "- More careful hyperparameter optimization \n",
    "    - Because of limited time, we did not carry out extensive hyperparameter optimization. For instance, we didn't carry out hyperparameter optimization with the XGBoost model. It might be worth exploring this area a bit more.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## 14. Your takeaway <a name=\"15\"></a>\n",
    "<hr>\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from the supervised machine learning material we have learned so far? Please write thoughtful answers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLEASE READ BEFORE YOU SUBMIT:** \n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from \"1\" will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "4. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. \n",
    "5. Make sure that the plots and output are rendered properly in your submitted file. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb so that the TAs can view your submission on Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a tricky one but you did it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
